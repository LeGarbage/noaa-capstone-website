import { read } from 'text-kit';
import todoKeywordSet from '../todo-keyword-set.js';
import block from './block.js';
import latex from './latex.js';
import comment from './comment.js';
import drawer from './drawer.js';
import footnote from './footnote.js';
import headline from './headline.js';
import hr from './hr.js';
import { tokenize as inlineTok } from './inline/index.js';
import keyword from './keyword.js';
import listItem from './list.js';
import planning from './planning.js';
import table from './table.js';
import emptyLines from './empty.js';
const PLANNING_KEYWORDS = ['DEADLINE', 'SCHEDULED', 'CLOSED'];
export const tokenize = (text, options) => {
    const { timezone, todos, range } = options;
    const reader = read(text, range);
    const { getChar } = reader;
    const globalTodoKeywordSets = todos.map(todoKeywordSet);
    const inBufferTodoKeywordSets = [];
    function todoKeywordSets() {
        return inBufferTodoKeywordSets.length === 0
            ? globalTodoKeywordSets
            : inBufferTodoKeywordSets;
    }
    let tokens = [];
    let cursor = 0;
    const tokenizers = [
        ({ getChar, eat }) => getChar() === '\n' && {
            type: 'newline',
            position: eat('char').position,
        },
        headline(todoKeywordSets),
        drawer,
        planning({ keywords: PLANNING_KEYWORDS, timezone }),
        keyword,
        block,
        latex,
        listItem,
        comment,
        table,
        hr,
        footnote,
    ];
    function tok() {
        const all = emptyLines(reader);
        // eat('whitespaces')
        if (!getChar())
            return all;
        for (const t of tokenizers) {
            const result = t(reader);
            if (!result)
                continue;
            const tokens = Array.isArray(result) ? result : [result];
            if (tokens.length > 0) {
                return [...all, ...tokens];
            }
        }
        // last resort
        const currentLine = reader.read({ end: reader.endOfLine() });
        const inlineTokens = inlineTok(currentLine);
        reader.jump(currentLine.now());
        return [...all, ...inlineTokens];
    }
    const peek = (offset = 0) => {
        const pos = cursor + offset;
        if (pos >= tokens.length) {
            tokens = tokens.concat(tok());
        }
        return tokens[pos];
    };
    const modify = (f, offset = 0) => {
        const pos = cursor + offset;
        const token = peek(offset);
        if (token !== undefined) {
            tokens[pos] = f(token);
        }
    };
    const _eat = (type = undefined) => {
        const t = peek();
        if (!t)
            return undefined;
        if (!type || type === t.type) {
            cursor += 1;
            return t;
        }
        return undefined;
    };
    return {
        peek,
        eat: _eat,
        eatAll(type) {
            let count = 0;
            while (_eat(type)) {
                count += 1;
            }
            return count;
        },
        match(cond, offset = 0) {
            const token = peek();
            if (!token)
                return false;
            if (typeof cond === 'string') {
                return token.type === cond;
            }
            return cond.test(token.type);
        },
        all(max = undefined) {
            let _all = [];
            let tokens = tok();
            while (tokens.length > 0) {
                _all = _all.concat(tokens);
                tokens = tok();
            }
            return _all;
        },
        save: () => cursor,
        restore(point) {
            cursor = point;
        },
        addInBufferTodoKeywords(text) {
            inBufferTodoKeywordSets.push(todoKeywordSet(text));
        },
        substring: (pos) => reader.substring(pos.start, pos.end),
        modify,
        get now() {
            const token = peek();
            return reader.toIndex(token?.position.start ?? Infinity);
        },
        toOffset: (point) => reader.toIndex(point),
        toPoint: (offset) => reader.toPoint(offset),
    };
};
