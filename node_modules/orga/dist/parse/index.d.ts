import { Lexer } from '../tokenize';
import { Node } from 'unist';
import { Document, Parent, Token } from '../types.js';
import { Context } from './context.js';
import { ParserOptions } from '../options';
export type Parse = (lexer: Lexer) => Parent | undefined;
type FlowControl = 'break' | 'next' | 'finish';
export type Action = (token: Token, context: Context) => FlowControl | Handler | void;
export type Predicate = string | 'EOF' | RegExp | ((token: Node) => boolean);
export declare function test(node: Node, predicate: Predicate): boolean;
export declare function not(test: Predicate): Predicate;
export declare function toFunc(test: Predicate): (token: Node) => boolean;
type Rule = {
    test: Predicate | Predicate[];
    action: Action | Handler;
};
export interface Handler {
    name: string;
    rules: Rule[];
    eof?: (context: Context) => void;
}
export interface Parser {
    advance: () => Document | number;
    parse: () => Document;
    finish: () => Document;
}
export declare function parser(lexer: Lexer, options: ParserOptions): Parser;
export {};
